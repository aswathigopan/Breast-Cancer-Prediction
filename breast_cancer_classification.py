# -*- coding: utf-8 -*-
"""Breast Cancer Classification_Group IV_Mini Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RFdQMbG0AQOvI9WqfxPo8g36_o5xBKtg

### **BREAST CANCER STAGE PREDICTION**

Import required libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,classification_report

"""Import Dataset"""

from google.colab import files

files.upload()

data=pd.read_csv('data.csv')

cancer_data=data.copy(deep=True)

"""## **Data Processing & Analyzing**"""

cancer_data.shape

cancer_data.head()

cancer_data.tail()

cancer_data.isnull().sum()

cancer_data.isna().sum()

cancer_data.info()

"""Unnamed:32 have 569 NaN values ,it means that we cannot use this column for our analysis and the column 'id'is also not required.So we can drop these two columns"""

cancer_data.drop(['id','Unnamed: 32'],axis=1,inplace=True)

cancer_data.shape

cancer_data.head()

"""Encode the categorical data. Change the values in the column ‘diagnosis’ from M and B to 1 and 0 respectively"""

cancer_data['diagnosis']=cancer_data['diagnosis'].map({'M':1,'B':0})

cancer_data.tail()

cancer_data.describe()

"""**Finding the corelation of the features**

Here we have 30 column features divided into three parts :Mean, Stranded Error and Worst .Each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension)

Corelation of parameters under Mean:
"""

cancer_data.columns

mean_features=['radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

corr1 = cancer_data[mean_features].corr()

plt.figure(figsize=(10,10))
sns.heatmap(corr1,cbar = True,square = True,annot=True, fmt= '.2f',annot_kws={'size': 10},xticklabels= mean_features, yticklabels= mean_features,cmap='YlGnBu')

"""The radius_mean, perimeter_mean and area_mean are highly correlated as expected from their relation so from these we will use anyone of them. The compactness_mean, concavity_mean and concavepoint_mean are highly correlated so we will use compactness_mean from here. so selected Parameter for use is radius_mean,texture_mean,smoothness_mean,compactness_mean, symmetry_mean and fractal_dimension_mean."""

sns.scatterplot(x='radius_mean',y='perimeter_mean',data=cancer_data,hue ="diagnosis",edgecolor='grey')

"""Corelation of parameters under Standard Error:"""

se_features=['radius_se','texture_se','perimeter_se', 'area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se',]

corr2 =cancer_data[se_features].corr() 
plt.figure(figsize=(10,10))
sns.heatmap(corr2, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},xticklabels= se_features, yticklabels= se_features,cmap='coolwarm')

"""The radius_se, perimeter_se and area_se are highly correlated as expected from their relation so from these we will use anyone of them. The compactness_se, concavity_se and concavepoint_se are highly correlated so we will use compactness_se from here. so selected Parameter for use is radius_se,texture_se,smoothness_se,compactness_se, symmetry_se and fractal_dimension_se."""

sns.scatterplot(x='compactness_se',y='concavity_se',data=cancer_data,hue ="diagnosis",edgecolor='grey')

"""Corelation of parameters under Worst:"""

worst_features=['radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']

corr3 = cancer_data[worst_features].corr() 
plt.figure(figsize=(14,14))
sns.heatmap(corr3, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},xticklabels= worst_features, yticklabels= worst_features,cmap='BuPu')

"""The radius_worst, perimeter_worst and area_worst are highly correlated as expected from their relation so from these we will use anyone of them. The compactness_worst, concavity_worst and concavepoint_worst are highly correlated so we will use compactness_worst from here. so selected Parameter for use is radius_worst,texture_worst,smoothness_worst,compactness_worst, symmetry_worst and fractal_dimension_worst.

Drop the unnecessary columns
"""

cancer_data.drop(['perimeter_mean','area_mean','concavity_mean','concave points_mean','perimeter_se','area_se', 'concavity_se', 'concave points_se','perimeter_worst','area_worst','concavity_worst', 'concave points_worst'],axis=1,inplace=True)

cancer_data.shape

cancer_data.head()

"""Distribution of target parameter"""

sns.countplot(x=cancer_data['diagnosis'])

cancer_data['diagnosis'].value_counts()

"""Distribution of features"""

cancer_data.iloc[:,1:18].hist(figsize=(10,12))
plt.tight_layout()

"""Almost all the feature parametes follow somewhat normal distributions"""

sns.pairplot(cancer_data,hue='diagnosis')

"""Checking the outliers"""

cancer_data.columns

mean_data = pd.melt(cancer_data,id_vars = "diagnosis",value_vars = ['radius_mean','texture_mean'])
plt.figure(figsize = (10,5))
sns.boxplot(x = "variable", y ="value", hue="diagnosis",data= mean_data)
plt.show()

mean_data1 = pd.melt(cancer_data,id_vars = "diagnosis",value_vars = ['smoothness_mean','compactness_mean','symmetry_mean','fractal_dimension_mean'])
plt.figure(figsize = (13,10))
sns.boxplot(x = "variable", y ="value", hue="diagnosis",data= mean_data1)
plt.show()

se_data = pd.melt(cancer_data,id_vars = "diagnosis",value_vars = ['radius_se','texture_se'])
plt.figure(figsize = (10,5))
sns.boxplot(x = "variable", y ="value", hue="diagnosis",data= se_data)
plt.show()

se_data1 = pd.melt(cancer_data,id_vars = "diagnosis",value_vars = ['smoothness_se','compactness_se','symmetry_se','fractal_dimension_se'])
plt.figure(figsize = (13,10))
sns.boxplot(x = "variable", y ="value", hue="diagnosis",data= se_data1)
plt.show()

"""Instead of boxplot we can use violinplot,swarmplot ...."""

worst_data = pd.melt(cancer_data,id_vars = "diagnosis",value_vars = ['radius_worst','texture_worst'])
plt.figure(figsize = (10,5))
sns.violinplot(x = "variable", y ="value", hue="diagnosis",data= worst_data)
plt.show()

worst_data1 = pd.melt(cancer_data,id_vars = "diagnosis",value_vars = ['smoothness_worst','compactness_worst','symmetry_worst','fractal_dimension_worst'])
plt.figure(figsize = (15,10))
sns.boxplot(x = "variable", y ="value", hue="diagnosis",data= worst_data1)
plt.show()

"""There are not much outliers in the features,so we can proceed with the data

Corrrelation between the features and target
"""

cancer_data.corr()

"""By looking to the first column its clear that the features texture_se and symmetry_se is not much correlated to our target 'diagnosis'.So that we can drop these two features."""

cancer_data1=cancer_data.drop(['texture_se','symmetry_se'],axis=1)

cancer_data1.shape

cancer_data1.head()

"""Creating features and target"""

x1=cancer_data1.drop('diagnosis',axis=1)
y1=cancer_data1['diagnosis']

"""Standardisation of features"""

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()

x1=sc.fit_transform(x1)

"""Seperating train and test data"""

x1_train,x1_test,y1_train,y1_test=train_test_split(x1,y1,test_size=0.30,random_state=60)

x1_train.shape

y1_train.shape

x1_test.shape

y1_test.shape

"""## MODELLING_Logistic Regression"""

from sklearn.linear_model import LogisticRegression
log=LogisticRegression(class_weight={0:0.5,1:0.5})

model_log=log.fit(x1_train,y1_train)

y_pred_log=model_log.predict(x1_test)

confusion_matrix(y1_test,y_pred_log)

accuracy_score(y1_test,y_pred_log)

print(classification_report(y1_test,y_pred_log))

"""###MODELLING _Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier(class_weight={0:0.4,1:0.6})

model_dt=dt.fit(x1_train,y1_train)

y_pred_dt=model_dt.predict(x1_test)

confusion_matrix(y1_test,y_pred_dt)

accuracy_score(y1_test,y_pred_dt)

print(classification_report(y1_test,y_pred_dt))

"""## MODELLING_Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf=RandomForestClassifier(n_estimators=350,class_weight={0:0.5,1:0.5})

model_rf=rf.fit(x1_train,y1_train)

y_pred_rf=model_rf.predict(x1_test)

confusion_matrix(y1_test,y_pred_rf)

accuracy_score(y1_test,y_pred_rf)

print(classification_report(y1_test,y_pred_rf))

x=cancer_data1.drop('diagnosis',axis=1)
y=cancer_data1['diagnosis']

pd.DataFrame(rf.feature_importances_,index=x.columns)

plt.bar(range(len(rf.feature_importances_)),rf.feature_importances_)
plt.show()

"""### MODELLING_Bagging Classifier"""

from sklearn.ensemble import BaggingClassifier

bc=BaggingClassifier(n_estimators=350,base_estimator=dt,random_state=60)

model_bc=bc.fit(x1_train,y1_train)

y_pred_bc=model_bc.predict(x1_test)

confusion_matrix(y1_test,y_pred_bc)

accuracy_score(y1_test,y_pred_bc)

print(classification_report(y1_test,y_pred_bc))

"""## MODELLING_AdaBoostClassifier"""

from sklearn.ensemble import AdaBoostClassifier

ada=AdaBoostClassifier(base_estimator=dt,n_estimators=350,random_state=60)

model_ada=ada.fit(x1_train,y1_train)

y_pred_ada=model_ada.predict(x1_test)

confusion_matrix(y1_test,y_pred_ada)

accuracy_score(y1_test,y_pred_ada)

print(classification_report(y1_test,y_pred_ada))

"""## MODELLING_Gradient Boosting Classifier"""

from sklearn.ensemble import GradientBoostingClassifier

gb=GradientBoostingClassifier(subsample=0.8,max_features=0.8,n_estimators=350)

model_gb=gb.fit(x1_train,y1_train)

y_pred_gb=model_gb.predict(x1_test)

confusion_matrix(y1_test,y_pred_gb)

accuracy_score(y1_test,y_pred_gb)

print(classification_report(y1_test,y_pred_gb))

"""## MODELLING_Extreme Gradient Boosting Classifier"""

from xgboost import XGBClassifier

xgb=XGBClassifier(learning_rate=0.1,n_estimators=90)

model_xgb=xgb.fit(x1_train,y1_train)

y_pred_xgb=model_xgb.predict(x1_test)

confusion_matrix(y1_test,y_pred_xgb)

accuracy_score(y1_test,y_pred_xgb)

print(classification_report(y1_test,y_pred_xgb))

"""## MODELLING_KNN"""

from sklearn.neighbors import KNeighborsClassifier

"""Calculating the error value for k values between 1 & 20"""

Misclassified_samples=[]
for i in range(1,20):
  kn=KNeighborsClassifier(n_neighbors=i)
  kn.fit(x1_test,y1_test)
  pred=kn.predict(x1_test)
  Misclassified_samples.append((y1_test != pred).sum())
print(Misclassified_samples)

"""Here the misclassification is less when the k value is 3"""

knn=KNeighborsClassifier(n_neighbors=3)

model_knn=knn.fit(x1_train,y1_train)

y_pred_knn=model_knn.predict(x1_test)

confusion_matrix(y1_test,y_pred_knn)

accuracy_score(y1_test,y_pred_knn)

print(classification_report(y1_test,y_pred_knn))

"""## MODELLING_SVM"""

from sklearn.svm import SVC

sv=SVC(kernel='linear',random_state=43)

model_sv=sv.fit(x1_train,y1_train)

y_pred_sv=model_sv.predict(x1_test)

confusion_matrix(y1_test,y_pred_sv)

accuracy_score(y_test,y_pred_sv)

print(classification_report(y1_test,y_pred_sv))

"""## **Applying Dimensionality Reduction_PCA**

Instead of applynig PCA directly to the whole data,we and fist divide the features into three components:mean,se & worst.We will apply PCA on these three components and create a new dataframe.
"""

cancer_data1.columns

mean_data=cancer_data1[['radius_mean','texture_mean','smoothness_mean','compactness_mean','symmetry_mean','fractal_dimension_mean']]

se_data=cancer_data1[['radius_se','smoothness_se','compactness_se','fractal_dimension_se']]

worst_data=cancer_data1[['radius_worst','texture_worst','smoothness_worst','compactness_worst','symmetry_worst','fractal_dimension_worst']]

"""Before applying PCA,first standardize the data"""

mean_data=sc.fit_transform(mean_data)
se_data=sc.fit_transform(se_data)
worst_data=sc.fit_transform(worst_data)

from sklearn.decomposition import PCA
pca=PCA()

"""Applying PCA on 'mean' parameters"""

mean_d1=pca.fit(mean_data)

mean_d1.explained_variance_ratio_

"""From the 6 principal components ,take the top 4 components for analysis."""

pca1=PCA(n_components=4)
mean_pca=pca1.fit_transform(mean_data)

"""Applying PCA on 'se' parameters"""

se_d1=pca.fit(se_data)

se_d1.explained_variance_ratio_

"""From the 4 principal components ,take the top 3 components"""

pca2=PCA(n_components=3)
se_pca=pca2.fit_transform(se_data)

"""Apply PCA on 'worst' parameters"""

worst_d1=pca.fit(worst_data)

worst_d1.explained_variance_ratio_

"""From the 6 principal components, take the top 4  components"""

pca3=PCA(n_components=4)
worst_pca=pca3.fit_transform(worst_data)

"""Create a new data frame using the selected principal components"""

df1=pd.DataFrame(data=mean_pca,columns=[['PC1','PC2','PC3','PC4']])
df2=pd.DataFrame(data=se_pca,columns=[['PC5','PC6','PC7']])
df3=pd.DataFrame(data=worst_pca,columns=[['PC8','PC9','PC10','PC11']])

cancer_pca=pd.concat([df1,df2,df3,cancer_data1['diagnosis']],axis=1)

cancer_pca.shape

cancer_pca.head()

"""Create the features and target"""

x=cancer_pca.drop(['diagnosis'],axis=1)
y=cancer_pca['diagnosis']

x.shape

y.shape

"""Split the train and test data"""

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=60)

"""## MODELLING_Logistic Regression"""

model_pca_log=log.fit(x_train,y_train)

ypca_pred_log=model_pca_log.predict(x_test)

confusion_matrix(y_test,ypca_pred_log)

accuracy_score(y_test,ypca_pred_log)

print(classification_report(y_test,ypca_pred_log))

"""## MODELLING_Decision Tree"""

model_pca_dt=dt.fit(x_train,y_train)

ypca_pred_dt=model_pca_dt.predict(x_test)

confusion_matrix(y_test,ypca_pred_dt)

accuracy_score(y_test,ypca_pred_dt)

print(classification_report(y_test,ypca_pred_dt))

"""## MODELLING_Random Forest"""

model_pca_rf=rf.fit(x_train,y_train)

ypca_pred_rf=model_pca_rf.predict(x_test)

confusion_matrix(y_test,ypca_pred_rf)

accuracy_score(y_test,ypca_pred_rf)

print(classification_report(y_test,ypca_pred_rf))

"""# MODELLING_BaggingClassifier"""

model_pca_bc=bc.fit(x_train,y_train)

ypca_pred_bc=model_pca_bc.predict(x_test)

confusion_matrix(y_test,ypca_pred_bc)

accuracy_score(y_test,ypca_pred_bc)

print(classification_report(y_test,ypca_pred_bc))

"""## MODELLING_AdaBoost Classifier"""

model_pca_ada=ada.fit(x_train,y_train)

ypca_pred_ada=model_pca_ada.predict(x_test)

confusion_matrix(y_test,ypca_pred_ada)

accuracy_score(y_test,ypca_pred_ada)

print(classification_report(y_test,ypca_pred_ada))

"""## MODELLING_Gradient Boosting Classifier"""

model_pca_gb=gb.fit(x_train,y_train)

ypca_pred_gb=model_pca_gb.predict(x_test)

confusion_matrix(y_test,ypca_pred_gb)

accuracy_score(y_test,ypca_pred_gb)

print(classification_report(y_test,ypca_pred_gb))

"""## MODELLING_Extreme Gradient Boosting"""

model_pca_xgb=xgb.fit(x_train,y_train)

ypca_pred_xgb=model_pca_xgb.predict(x_test)

confusion_matrix(y_test,ypca_pred_xgb)

accuracy_score(y_test,ypca_pred_xgb)

print(classification_report(y_test,ypca_pred_xgb))

"""# MODELLING_KNN"""

model_pca_knn=knn.fit(x_train,y_train)

ypca_pred_knn=model_pca_knn.predict(x_test)

confusion_matrix(y_test,ypca_pred_knn)

accuracy_score(y_test,ypca_pred_knn)

print(classification_report(y_test,ypca_pred_knn))

"""## MODELLING_SVM"""

model_pca_sv=sv.fit(x_train,y_train)

ypca_pred_sv=model_pca_sv.predict(x_test)

confusion_matrix(y_test,ypca_pred_sv)

accuracy_score(y_test,ypca_pred_sv)

print(classification_report(y_test,ypca_pred_sv))